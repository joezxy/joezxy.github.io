---
layout: post
category: "read"
title: "In searching an understandbale consensus algorithm"
tags: [consensus, raft]
---


### 1 Introduction
一致性算法允许一组机器以一个coherent group的形式工作，其可以在某些成员故障时继续工作。因为这样的特征，一致性算法对于构建大规模可靠软件系统是非常关键的。在上个世纪，Paxos主宰着一致性算法的讨论：大多数一致性的实现都是基于Paxos的，或者是受其影响的，在教学中，Paxos也被用于讲解一致性的技术。

不幸的是，Paxos非常难以理解，虽然已经有大量的尝试想要让它更加易懂。而且，它的架构需要复杂的变更，以支持实际系统的实现。带来的结果是，Paxos系统构建人员和学生都很困难。

我们在和Paxos艰苦搏斗之后，做出一个决定，去研究一种新的一致性算法，其能够为系统构建人员和学生提供一个更好的基础。我们的首要目标是：可理解性Understandability。此外，我们希望此算法不仅仅是能够正常运行，而且可以清晰的知道为什么可以运行。

工作输出是一个一致性算法：Raft。在Raft的设计过程中我们使用了专门的技巧来提升可理解性，包括分解decomposition（Raft将选主、日志复制和安全进行了分离）以及状态空间减少（相对于Paxos，Raft减少了非确定性的度，以及服务器之间不一致的可能性）。曾经对两所大学的43位学生进行了调查，结果表明Raft比Paxos容易理解的多：在学习两个算法之后，他们中的33个对于Raft理解的更好。

Raft和现有的一致性算法相似（尤其是，Oki和Liskov的Viewstamped Replication），但是它有几个新颖之处：
Strong Leader：Raft使用了更强形式的Leadership。例如，日志条目只会从leader到其他节点。这简化了日志复制的管理难度，也使得Raft更加易于理解。
Leader Election：Raft使用了一个随机计时器来选主。为了实现它，仅仅需要在已有的心跳机制上稍作增强，同时可以简单快速的处理冲突
Membership change：Raft使用了一个新的joint consesus方法来处理集群内服务器变化的情况，「TODO」

我们相信Raft优于Paxos和其他一致性算法，无论是从教学目的还是作为实现的基础来看。它更加简单，而且描述了实现系统所需的足够多的细节信息，而且它已经有了多个开源实现，也在多个公司得到应用，它的安全性已经被正式证明，而且其性能也与其他一致性算法相当。

### 2 Replicated State Machine
一致性算法通常是在复制状态机的上下文中提出的。在复制状态机的方法中，一组机器上的状态机计算得到相同的状态副本，并且在一些机器失效后，还能继续工作。在分布式系统中，复制状态机用于解决大量的容错问题。例如，大规模系统（如GFS、HDFS、RAMCloud等）有一个集群leader，通常使用一个独立的复制状态机来管理选主和配置数据存储，在主结点失效后，其还能继续工作。复制状态机的例子包括Chubby和ZooKeeper。

复制状态机通常使用日志复制来实现，如图1所示。每个服务器存储一个日志，包括一系列的命令，每个服务器的状态机会依次执行它。每个日志包含了相同顺序的命令。因为状态机是确定性的，每个服务器执行相同的运算，将会得到相同顺序的输出。

保持复制日志的一致性是一致性算法的工作。服务器中的一致性模块从客户端接收命令，并将其添加到日志中。它会和其他机器上的一致性模块进行通信，来确保每台机器上的日志都包含了相同顺序的相同请求，即使是在出现了机器故障的情况下。一旦命令被正确复制后，每台服务器上的状态机都以相同的日志顺序来进行处理，最终将结果返回到客户端。其结果是，对外呈现出单个高可靠的状态机。

实际系统中的一致性算法通常有着下面的特性：
在非拜占庭环境（允许网络延迟、分区、丢包、重复包、包乱序）下，保证safety（永远不返回错误结果）
只要任意多数机器可工作且能够相同通信，以及与客户端通信，整个系统就是完全可工作的。因此，一个五节点的典型集群可以容忍最多两节点实效。
不依赖与timing来保证日志的一致性：在最坏情况下，错误的时钟和极端的消息延迟可能会带来可用性问题。「TOTHINK」
在通常情况下，一条命令可以在集群的多数节点完成rpc的响应后结束；少数运行缓慢的节点不会影响整体系统的性能。

### 3 What"s wrong with Paxos
在过去的十年间，Paxos几乎就是一致性的同义词。课堂上主要教的就是这个协议，大量的一致性实现也以Paxos作为起点。Paxos先是定义了可以得到单个共识decision（如单个复制日志条目）的协议，称之为single-decree Paxos。进一步的，Paxos可以合并这个协议的多个实例来得到一系列的共识。称之为multi-Paxos。Paxos同时保证了safety和liveness，它同时也支持集群成员的变更。它的正确性已经被证明，它的效率处于中游水准。

不幸的是，Paxos有两个显著的缺点。首先，Paxos非常难以理解。协议的完整描述（The Part-time Parliament）没有多少人在努力尝试后能够成功理解。因此有了许多简化描述的解释版本（Pax o s made Simple、How to build a highly available system using consensus、The ACBD"s of Paxos）。这些解释版本都只针对于single-decree的子集，即便是这样，它们还是非常有挑战性的。在NSDI 2012中的非正式调查显示，我们发现没有多少人掌握了Paxos。我们自己的学习过程也很痛苦，直到读了多篇简化解释，以及设计了我们自己的协议之后，我们才真正理解了整个协议，这花费了将近一年的时间。

我们认为Paxos的不透明性是由于它选择将single-decree子集作为自己的基础。Single-decree Paxos是dence且subtle的：它分为两个阶段，没有简单直观的解释，无法独立的理解。因此，想要理解single-decree协议为何能够正确工作是相当困难的。Multi-Paxos的合并规则增添了额外的复杂度。我们相信多个共识的一致性问题可以使用其他形式分解，从而更加直接，更加明显。

Paxos的第二个问题是它并没有为构建实际系统提供一个良好的基础。一个原因是并没有广泛接受的multi-paxos算法，Lamport的描述中大多数是关于single-decree paxos的；它给出了multi-paxos的大体方法，但是缺乏大量细节。也有一些优化Paxos的尝试，但是它们都各不相同，而且与Lamport的方法也不相同。Chubby这样的系统实现了类Paxos的算法，而且细节信息也没有公开

此外，对于构建实际系统，Paxos架构并不好；这同样是由于single-decree分解造成的。例如，独立的选择一组日志条目，并将其融合成一个顺序日志中，这样并没有多少好处，只是增加了复杂性。如果将系统设计成基于顺序递增日志，则是相当简单高效的。另一个问题是Paxos使用了对等的P2P模式（虽然它最终也推荐使用弱leadership来优化性能）。对于只有一个共识要达成的简单场景这是有意义的，但是几乎没有系统使用这个方法，如果先选出一个leader，再让leader来协调共识则更加简单和高效。

这些问题导致的结果是，实际系统和Paxos之间只有很少的相似性。每个实现都从Paxos开始，在实现中发现其困难性，然后开发出一个大不相同的架构。这是非常耗时和易错的，同时Paxos的难以理解更加剧了这个问题。Paxos的形式化对于正确性的理论证明可能是比较好的，但是实际实现和Paxos的差异也使得这个证明的价值变得不大了。Chubby实现的评论是很典型的："Paxos算法描述与实际系统的需求之间存在着巨大的Gap。。。最终系统将基于一个未经证明的协议"

由于这些问题，我们认为Paxos并没有为系统构建和教学提供一个良好的基础。由于一致性在大规模软件系统中的重要性，我们决定设计一种优于Paxos的新一致性协议。Raft就是我们工作的成果。

### 4 Designing the understandability
在设计Raft时有许多目标：它必须为系统构建提供一个完整可行的基础，以显著降低开发人员的设计工作量；它必须在所有条件下保证safety，且在典型运行环境下可用；同时它也需要保证普通操作的效率。但是我们的最重要的目标是——也是最困难的挑战——可理解性。必须能让大多数读者轻松的理解算法。

当我们在不同方案中进行选择时，我们会使用可理解性作为方案评估的要素：解释各备选方案的难度如何？读者是否可以完全的理解方案以及暗示。

我们使用了两个技巧。第一个是广为人知的"问题分解"：只要可能，我们就将问题分解成可独立求解、解释、理解的子问题。例如，在Raft中，我们将选主、日志复制、safety以及成员变更进行了分解。

我们的第二个技巧是通过简化需考虑的状态数量，来简化状态空间，以使得系统更加清晰，并尽可能的消除非确定性。特别的，日志不允许存在hole，Raft限制了日志之间不一致的可能性。尽管在大多数情况下，我们尝试去消除非确定性，但是在某些场景下，非确定性却可以提升可理解性。特别的，随机的方式带来了非确定性，但是它们却使用了简单的方式来处理所有可能的选择，这样可以简化状态空间。（"choose any； it does not matter"）。我们使用随机的方式来简化Raft的选主算法。


### 5 The Raft consensus algorithm
Raft是一个管理复制日志的算法。图2中对算法进行了浓缩总结，图3列出了算法的关键属性。

Raft协议先选出一个leader，而后赋予此leader完全的责任来管理复制日志。leader从客户端接受日志条目，将它们复制到其他服务器，以及告知服务器何时可以安全的将日志应用到它们的状态机。有leader这一角色可以简化复制日志的管理。例如，leader可以决定在日志的哪里放置新的条目，而不需要与其他服务器进行交互，而且数据流也只是简单的从leader到其它服务器。在leader失效或者与其他服务器失去连接时，将会选出一个新的leader。

通过这样的leader机制，Raft将一致性问题分解成三个相对独立的子问题，下面将分别讨论：
Leader Election：当现有的Leader失效后，必须选出一个新的Leader（5.2）
Log Replication：Leader从客户端接收日志条目，将其复制到集群中，并强制其他的日志保持一致（5.3）
Safety：Raft的关键Safety特性是图3中的状态机Safety特性：如果任何服务器在其状态机上应用了一个特定的日志条目，则没有其他的服务器可以以相同的日志序号应用一个其他的命令。5.4描述了Raft是如何保证这一特性的；此解决方案基于5.2的选主机制增加了一个额外的限制。

在描述完一致性算法后，本章讨论了可用性的问题，以及系统中的timing的角色。

#### 5.1 Raft basic
一个Raft集群包含了多个服务器；典型配置是5台，这使得系统可以容忍两台机器失效。在任何时刻，每个服务器都处于三种状态之一：leader、follower和candidate。在一般场景下，只有一个leader，所有其他服务器都是follower。Follower是被动的：它们自己不会发起请求，只会简单的响应leader和candidate的请求。leader处理所有的客户端请求（如果客户端与一个follower交互，follower会将其重定向给leader）。第三种状态，candidate，用于选出一个新的 leader。图4显示了这些状态，以及它们之间的转移关系。

Raft将时间分隔成任意长度的term，如图5所示。term使用连续的整数进行标识。每个term都以一个选主开始，此时一个或多个candidate尝试成为leader。如果一个candidate成功被选主，则它将在此term的其余时间内作为leader。在某些情况下，选举将可能形成平票，此时term将以没有leader而结束；一个新的term将会马上开始，并进行一个新的选主。Raft保证在每个term中最多只有一个leader。

不同的服务器可能会看到不同时间下的term之间的转换，在某种情况下，一个服务器可能不会看到某个选主，甚至是整个term。在Raft中，term就像一个逻辑时钟，它让服务器可以检测到废弃的信息，比如过时的leader。每个服务器都存储一个current term数字，它是单调递增的。每当服务器进行通信时都会交换current term；如果一个服务器的current term小于另一个服务器的，则它会将自己的current term更新到较大的那个值。如果一个candidate或者leader发现自己的term已经过时了，它将立即转换为follower状态。如果一个服务器收到了过时的term数字，它将拒绝这一请求。

Raft使用RPC进行通信，基础的一致性算法只需要两类RPC。RequestVote RPC由candidate在选主时发起，AppendEntries RPC由Leader发起，用于复制日志条目，并提供心跳。第7章将引入第三类RPC来在服务器之间传递快照。如果服务器没有及时收到响应，则会重试RPC，同时服务器也会并行的发起RPC以获得更好的性能。

#### 5.2 Leader election
Raft使用了心跳机制来触发选主。当服务器启动时，它先作为follower。一个服务器会一直保持为follower状态，直到它收到来自leader或candidate的有效的RPC。Leader周期性的发送心跳（没有日志条目的AppendEntries RPC）给所有的follower，以维护它们的授权。如果一个 follower在election timeout内没有收到请求，则它认为当前已经没有有效的leader了，需要开始选举出一个新的leader。

在开始选举时，follower将自己的term递增，将自己变为candidate状态。然后它为自己投票，向集群中的所有服务器发送一个 RequestVote RPC。candidate一直保持这一状态，直到下面的三种情况发生：（1）它赢得选主；（2）另一个服务器将自己选为了leader；（3）一段时间过去了，但并没有获胜者。

如果一个candidate收到了整个集群内的多数服务器发来的针对此term的响应，则它赢得此次选主。每个服务器最多只能为某一个term选一个candidate，其原则是first-come-first-server。多数原则保证了某个term最多只会有一个candidate能够获胜。一旦某个candidate获胜，它就变成leader。它将给其他的服务器发送心跳消息以建立它的授权，同时避免新的选举发生。

当正在等待投票时，某个candidate可能会收到其他声称为leader的服务器发来的AppendEntries RPC。如果leader的term（在其RPC中）大于等于candidate自己的当前term，则candidate承认此leader的合法地位，并将自己返回到follower角色。如果term中的term小于candidate自己的当前term，则candidate拒绝此RPC，继续保持自己的candidate角色。

第三种可能情况是candidate既没有赢得也没有输掉选举：如果多个follower都在相同的时间成为candidate，选票将会被分散，从而没有candidate可以获得多数选票。当发生这种情况时，每个candidate都会超时，并递增自己的term启动一个选举。但是，如果没有特殊的方法，这种选票分散会无限的重复下去。

Raft使用了随机的选举超时来保证选票分散很少发生，并且也可以快速解决这一问题。为了避免选票分散，选举超时在一个固定的间隔内随机选择（150－300ms）。这可以使得大多数情况下只有一个服务器会超时；它将在其他服务器超时之前赢得选举并发送心跳。相同的机制也用于处理选票分散。每个candidate在选举开始时重置自己的随机超时时间，这将降低在新选举中出现又一次选票分散的可能性。

选举是一个我们如何在不同的设计中考虑可理解性的例子。开始时我们想要使用ranking系统：每个candidate被分配一个唯一的rank，其被用于在竞争candidate中进行选择。如果一个candidate发现其他candidate有更高的rank，它将回到follower状态，从而拥有更高ranking的candidate可以在下次选举是更轻松的获胜。我们发现这一方法存在着可用性方面的复杂性（如果一个高rank服务器失效，则一个低rank需要超时并重新变成candidate，但如果出现的太快，则它可能会重置正在进行的选举过程）。我们几次修改了这一算法，但每次修改都会带来新的小问题。最终我们认识到随机的重试是更加明显和易理解的。

#### 5.3 Log Replication
一旦选出了Leader，它就开始服务客户端的请求了。每个客户端请求都包含了一个将被复制状态机所执行的命令。Leader将此命令作为一个新的条目附加到它的日志中，然后并行的向其他的服务器发送AppendEntries RPC。当此条目已经被安全的复制，Leader将此条目应用到自己的状态机，并返回执行结果给客户端。如果Follower故障或者运行过慢，或者出现网络丢包，Leader将一直重试AppendEntries RPC（即使是它已经给客户端响应了），直到所有的Follower都最终存储了所有的日志条目。

日志如图6那样的组织。每条日志条目存储一个状态机命令，以及收到此条目时的term编号。日志条目中的term编号被用于检测日志中的不一致，以及确保图3中的一些属性。每个日志条目也有一个整型的索引值，用于标识其在日志中的位置。

Leader决定如何可以安全的将一条日志条目安全的应用到状态机；这样的条目被成为committed。Raft保证committed的条目都是持久的，而且将最终被所有可用的状态机所执行。当Leader创建的条目被复制到服务器的多数，则可以被commit。它同时也将commit leader之前所有的日志，也包括了由之前leader创建的条目。Leader维护着它所知最高的索引，它将此索引包含在后面的AppendEntries RPC中，这样所有的服务器最终都可以获知。一旦一个Follower直到一个条目已经 committed，它就将此条目应用到它的本地状态机中。

Raft维护了下面的属性，其共同支撑了图3中的Log Matching属性：
如果不同日志中的两个条目有着相同的索引和条目，则它们存储了相同的命令。
如果不同日志中的两个条目有着相同的索引和条目，则日志中所有之前的条目都是相同的

第一条属性是因为一个leader在指定term、指定索引只会创建一个条目，而且日志条目也不会改变它们在日志中的位置。第二条属性是由AppenEntries执行的一个一致性检测。当发送一个AppendEntries RPC时，leader包含了其日志中在新条目之前的条目索引和term。如果follower在其日志中没有找到相同的term和索引，则它拒绝这个新的条目。一致性检测的步骤为：日志初始的空状态符合Log Matching属性，而且一致性检测保证了日志增加时仍然符合Log Matching属性。因此，当AppendEntries返回成功后，leader知道follower中新条目之前的日志和它的日志相同。

在普通操作下，leader和follower的日志是一致的，因此AppendEntries一致性检测不会失败。但是，leader死机将会使得日志变的不一致（老的leader可能没有完全复制它的所有日志条目）。这样的不一致可以由于一系列的leader和follower的死机造成。图7展示了follower的日志怎样会与新的leader不同。follower可能会缺少leader上的一些条目，它也可能有一个leader上面没有的条目。日志中缺失或额外的条目都可能会跨多个term。

在Raft中，leader通过强制follower复制自己的日志来解决不一致。这意味着follower日志中的冲突条目将被leader日志中的条目所覆盖。

为了使得follower的日志与自己的日志一致，leader必须找到两个日志认可的最新日志条目，删除follower上此条目之后的所有条目，并将leader上此条目之后的所有条目发送给follower。所有的这些动作都作为AppendEntries RPC执行的一致性检测的响应。leader为每个follower维护一个nextIndex，这是leader发送给follower的下一个日志条目的索引。当一个leader获得leader角色之初，它将所有的nextIndex值设置为比它的日志中最后条目的索引大一（图7中就是11）。如果follower的日志和leader的不一致，AppendEntries一致性检测将在下一个AppendEntries RPC时失败。在一次拒绝之后，leader将nextIndex递减，并重试AppendEntries RPC。最终nextIndex值将会到达一个leader和follower的日志都match的值。此时，AppendEntries将会成功，其将会删除所有follower上冲突的日志，并附加上leader的日志。

如果需要，可以优化协议以减少AppendEntries RPC被拒绝的次数。例如，当拒绝一个AppendEntries请求时，follower可以包含冲突条目的term，以及在此term中它存储的第一个索引。有了这个信息，leader可以直接跳过此term中的所有冲突条目；这样可以为一个term发送一个AppendEntries RPC，而不是每个条目一个RPC。实践中，我们怀疑这个优化是否必要，因为故障很少发生，出现很多不一致条目的可能性也不大。

有了这样的机制，一个leader不需要特别的处理，以在成为leader时恢复日志的一致性。它仅仅是开始普通的操作，在AppendEntries一致性检测失败时日志会自动的累积。Leader永远不会覆盖或删除自己日志中的条目。

这样的日志复制机制支撑了第2章中描述的期望的一致性协议：只要多数服务器在线，Raft就可以接受，复制和应用新的日志条目；在普通情况下，一个新条目可以在单轮RPC中完成多数服务器的复制；一个慢的follower也不会影响性能。

#### 5.4 Safety
前一章描述了Raft如何选主和复制日志条目。但是，想要保证每个状态机以相同的顺序执行相同的命令，这样的机制还不足够。例如，一个Follower可能会失效，同时Leader提交了许多日志记录，然后它可能被选为Leader，然后将这些条目覆盖掉了；其结果是，不同的状态机可能会执行不同的命令序列。

本章完成了Raft算法，增加了对于服务器被选举成Leader的限制。这个限制保证了任意指定的term的leader包含了之前term commit的所有条目（第三章中的Leader Completeness 属性）。有了这样的选举限制，我们可以让提交的规则更加的准确。最终，我们提供了Leader Completeness 属性的概要证明，以及这一属性如何保证复制状态机的正确行为。

##### 5.4.1 Election Restriction
在任何基于Leader的一致性算法中，Leader都最终存储所有的提交日志条目。在一些算法中（如Viewstamped Replication），一个Leader可以在其没有包含所有提交条目时被选为Leader。这些算法包含了额外的机制，以识别缺失的条目，并将其传送给新的Leader，可能是在选举的过程中，也可能在后续的操作中。但不幸的是这将带来额外的机制和复杂性。Raft使用了一个较简单的方法，它保证了所有之前term已提交的条目都会提供给新的Leader，而不需要向Leader传输这些条目。这意味着日志记录只会单向流动（从leader到follower），Leader从来不会覆盖它们日志中已有的条目。

Raft使用选举过程来放置一个没有包含所有已提交条目的Candidate赢得选举。一个将会选为Leader的Candidate必须与集群中的多数节点进行交互，这意味着每一条已提交的条目都至少存在于某一节点上。如果Candidate的日志和多数节点的日志的更新程度一样，则它就已经有了所有已提交的条目了。RequestVote RPC实现了这个限制：此RPC包含了Candidate日志的信息，如果投票者自己的日志比Candidate更新，则它会拒绝此投票。

Raft通过比较日志中最新条目的term和索引来判断两个日志哪个比较新。如果两个日志的最新条目有不同的term，则term较新的日志被认为是更新的。如果日志有着同样的条目，则较长的日志是更新的

##### 5.4.2 Committing Entries From Previous Terms
如5.3中描述的，一旦一个条目被存储到多数节点，则Leader可以从它的当前term知道此条目已经提交了。如果 Leader在提交一个条目时死机，后续的Leader将会尝试去结束此条目的复制。但是，一旦某个条目已经存储到多数节点，则一个 leader不能立即从之前的term知道此条目是否已经提交。图8展示了这样一个情况，一个老的日志条目被存储到多数节点，但是仍然被后续的leader覆盖的。

为了解决图8中的问题，Raft不会提交之前term的日志条目，只有leader当前term的日志条目会被提交，一旦当前term的一个条目以这种方式提交了，则由于Log Matching属性，所有之前的条目都被间接的提交了。有一些情况下，一个Leader可以安全的知道是否一个老的条目已经提交（例如，如果此条目已被存储在每个服务器上），但是出于简化的考虑，Raft使用了一个更加保守的方法。

Raft，因为日志条目保留了它们原始的term编号，。在其他的一致性算法中，如果一个新的leader复制之前term的条目，它必须




### 8 Client interaction
本章描述了客户端是如何和Raft交互的，包含了客户端如何找到集群Leader，以及 Raft怎样支持线性化的语义。所有的协同系统都需要考虑这些问题，Raft的方案和其他系统类似。

Raft的客户端将所有的请求都发送给Leader。当客户端启动时，它将随机的选择一个服务器进行连接。如果客户端选择的服务器不是Leader，则服务器会拒绝这个请求，并返回它所知的最新的Leader信息（AppendEntries请求中包含了Leader的网络地址）。如果一个Leader失效，客户端请求将会超时；然后客户端将会重新随机选择一个服务器进行重试。

Raft的目标是实现线性化的语义（每个操作都看起来是瞬间执行的，只执行一次，在调用它和产生响应之间）。但是，正如之前描述的，Raft可以多次执行某条命令：例如，如果Leader在提交条目之后，且返回客户端之前死机，客户端将使用一个新Leader来重试此命令，导致此命令被第二次执行。该问题的解决方案是为客户端的每个命令赋予一个唯一的序列号，然后状态机跟踪为每个客户端处理的最新序列号，并将其附加在相关的响应中。如果它收到了一个已经执行过的命令，它就不会执行此命令，而不会重新执行此请求。

处理只读操作可以不在日志中写入任何东西。但是，如果特殊的方案，它将可能返回过时的数据，因为响应此请求的Leader可能会被一个新的Leader所取代。线性化的读不能返回过时的数据，Raft需要特别注意两点，来在不使用日志的情况下保证线性化。首先，Leader必须有哪个条目已经提交的最新信息。Leader Completeness属性保证一个 Leader拥有所有的已提交条目，但是在term开始时，它可能还不知道。为了解决这一问题，它需要在此term中提交一个条目。Raft让每个Leader在term开始时，在日志中提交一个空的no-op条目。第二，在处理一个只读请求之前，Leader必须检查它是否已经还是Leader角色（如果一个更新的Leader被选出来，则原来的Leader的信息可能会过时。）Raft通过在响应只读请求之前，让leader与集群的多数节点交换心跳消息来解决这一问题。另一个备选方式是Leader可以依托心跳机制来提供某种形式的租约，但是这需要依赖于时间机制来达到安全性（它假定时钟偏移是有限的）


### 9 Implementation and Evaluation
我们使用Raft来保存RAMCloud中的配置信息，并且支撑了RAMCloud协调节点的失效恢复。Raft实现包含约2000行C++代码，不包含测试、备注、已经空白行。源代码免费可用。现在已有大概25个独立的第三方开源实现，基于本文的Draft版本。同时多个公司已经部署了基于Raft的系统。

「下文略」

### 10 Related work
有大量的协同算法的文献，可以分为下面的类别：
Lamport的初始Paxos的描述「15」，以及尝试对其的详细解释「16，20，21」
Paxos的演进，其提供了缺失的细节，并对算法进行了修改，以提供一个更好的实现基础「26，39，13」
实现了协同算法的系统，如Chubby「2，4」，Zookeeper「11，12」以及Spanner。Chubby和Spanner的算法的细节没有公布，虽然它们声称是基于Paxos的。Zookeeper公布了算法细节，但它和Paxos有较大的不同。
可以应用于Paxos的性能优化「18，19，3，25，1，27」
Oki和Liskov的Viewstamped Replication（VR），其原始的描述「29」和分布式事务的协议交织在一起，但是在最新的更新中，核心的协同算法已经被分离出来了。VR使用了基于Leader的方法，和Raft有些类似。

Raft和Paxos的最大不同在于Raft的强Leadership：Raf t使用Leader选举作为协同协议的一个关键部分，并且让Leader负责尽可能多的功能。这一方法带来了一个更为简单的算法。例如，在Paxos中，Leader选举和基础的协同协议是正交的：它只作为一个性能优化手段，且并不需要在达成协同时使用。但是这将带来额外的机制：Paxos包含了一个两阶段协议用于基本的协同操作，以及一个单独的机制用于选主。Raft与之不同，直接将选主嵌入到协同协议中，并用其作为两阶段协同的第一阶段。这样就可以使用比Paxos更少量的机制。












