---
layout: post
category: "read"
title:  "Omid IEEE 2014"
tags: [Database, Transaction]
---

### 1 Introduction
支持事务是DBMS的本质功能，如果不具备事务能力，只能被称为数据存储Data Store，而不是DBMS。

在SI隔离级别下，两个并发事务如果写同一个数据项，则它们是冲突的。冲突由系统检测，其中之一将被中止。冲突检测需要访问事务元数据，如事务的提交时间等。在决定一个事务是否可以读取数据的某个版本时也需要使用事务元数据。

由于大规模数据存储中有大量的事务，事务元数据常被分区存放在多个节点上，之前的做法是对存有事务元数据的节点加分布式锁，并使用2PC这样的协议。

这种方法的直接问题就是需要许多额外的节点来维护事务元数据。为了降低代价，Google Percolator将元数据和实际数据一同存储，也就是使用相同的数据服务器来维护元数据。这种方式会加重数据服务器的开销，虽然可以通过将消息打包批量处理来部分规避这个问题，但这样会给事务处理带来秒级的时延。这样的延迟对于Percolator的特定用例是可以接受的，但对于一般的OLTP应用，更希望的是低提交时延。
这种方法的另一个问题是如果一个事务在完成之前，客户端失效了，由此客户端加的分布式锁就会阻塞其他的事务。在Percolator中，这样未释放的锁可能会造成数分钟的延迟。

另一种方法是免锁机制，使用一个集中的TSO（Transaction Status Oracle）来监测事务的提交。SO维护了用于冲突检测以及读快照的事务元数据。客户端直接读写数据服务器，但是它们仍然需要访问SO的事务元数据来决定数据的那个版本对事务有效。虽然集中的方案可以避免分布式锁，但是单个节点有限的容量以及处理能力都对整个系统的可扩展性提出了挑战。

本文中，我们设计实现了一个免锁、集中的事务方案，为了减少事务元数据的内存占用，Omid将内存中的元数据信息进行了截断。此外，Omid还在客户端保存了事务元数据的一部分只读副本，这样客户端可以在本地决定数据存储中的哪个数据版本是对于事务有效的，从而降低了对于SO的访问压力。

### 2 Background
#### Sanpshot Isolation
SI是基于多版本数据库的乐观并发控制，其可以使得并发事务可以看到数据库的不同状态。SI保证一个事务的所有读取都是针对一个有效的数据库状态。为了实现SI，数据库维护了数据的多个版本，客户端的事务基于开始时间，可以看到数据的不同版本。SI的好处就是当一个事务进行写操作时不会阻塞其他事务的读操作。但是如果两个并发事务都对同一个数据项进行写操作的时候，也会产生冲突。SI的实现必须能够检测到这样的冲突，而且其中之一必须被回退。

为了实现SI，每个事务需要分配两个时间戳：一个在读取之前，另一个在提交修改的数据之前。在基于锁和免锁的方法中，时间戳都是使用集中的服务器（Timestamp Oracle）来分配的，因此提供了事务之间的提交顺序。事务ti的启动时间是Ts(ti)，提交时间是Tc(ti)，他可以看见自己的修改，或者是所有提交时间早于Ts(ti)的最近的版本。换句话说，事务可以看到所有自己的修改，以及在ti启动之前已提交的事务的修改。

如果ti没有与其他并发事务的写写冲突，它使用一个提交时间戳来提交修改。两个事务ti和tj是冲突的，如果符合下面两个条件：
	1）空间重叠：都对行r进行写操作；
	2）时间重叠：Ts(ti) < Tc (tj) and Ts (tj) < Tc (ti)

从空间和时间重叠的条件，我们知道一个SI的实现需要维护下面的事务元数据：
	1）Ts：事务启动时间戳的列表
	2）Tc：事务提交时间戳的列表
	3）writes：修改每行数据的事务列表

有下面两种在分布式数据存储中实现SI的方法

#### Distributed Implementation
在最朴素的SI分布式实现中，这三个事务列表可以被分布到客户端，每个客户端维护自己运行事务的那一部分事务元数据。因为这个列表在所有客户端上分布，因此每个客户端需要运行一个分布式协商算法（如2PC）来检测是否与所有其他客户端有写写冲突。这个方法的明显问题就是无法随着客户端进行扩展。而且需要为每个客户端提供容错机制，如果有一个客户端不响应了，整个系统就不能工作了。

为了解决上面的可扩展性问题，事务元数据可以在节点间进行分区。分布式协同算法只在事务涉及的分区上运行。Percolator优雅的实现了这种方案，事务元数据被分区存放在与数据相同的服务器上。未提交的数据直接写入主数据库。Ts列表简单的使用启动时间戳来维护，数据库通过存储数据的多个版本来维护writes列表。Percolator为每个Column Family增加了两列：lock和write。write列维护了Tc的列表，被分区存放于所有的数据服务器上。客户端运行一个2PC算法来在所有修改的数据项上更新这一列。lock列提供了细粒度的锁以供2PC算法使用。

虽然使用锁可以简化写写冲突的检测，一个失效或者很慢的事务却可以组织其他事务继续工作，直至锁得到释放。而且，维护事务元数据（增加的lock和write列）也将额外增加数据服务器的负担，为了规避，只能将发送给数据服务器的消息进行批量操作，从而造成事务处理的秒级延迟。（这里说的是Percolator）

#### Transaction Status Oracle
在SI的集中式实现中，单个服务器（SO）接收包含修改行ID（W）的提交请求。因为SO可以获知之前提交的修改行，它可以维护Tc和write列表，从而拥有足够的信息来检查每一行是否有时间重叠。在这里，我们展示了一个可能的SO抽象设计，其时间戳是从一个SO中集成的timestamp oracle来获取的，事务未提交的数据也直接存储在相同的数据表格中。和Percolator一样，timestamp oracle生成的也是唯一的时间戳，可以用作事务ID。这样就避免了在SO中维护Ts。

算法1描述了处理提交请求的流程。在算法1中，W是事务ti所有的修改行的列表，Tc和lastCommit都是SO的内存状态，包含了事务的提交时间戳、以及修改行的最后提交时间戳。请注意，lastCommit只是维护了writes列表的一个子集。

为了检查写写冲突，算法1与所有已提交事务检查是否存在时间重叠。换句话说，在写写冲突的情况下，算法将会提交首先接收到的提交请求。必须根据之前提交的所有事务的修改，来检查ti修改的每一个行r。第2行进行了这个检查，但只是对最后修改过行r的事务tl。可以归纳得知，这一检查可以涵盖了修改过行r的所有已提交事务。这一特性极大的简化了算法1，因为它只需要维护每行的最后提交时间（第8行）。同时可以注意，第2行只验证了时间重叠约束的第一部分（即只检查了待提交事务Ts(ti) < Tc(tj)，如果为真则说明事务ti启动后事务tj提交了修改，但并没有检查Tc(ti) > Ts(tj)），这是足够的，因为SO是从自身获取提交时间戳的，而不是像一般情况下，由客户端获取提交时间戳。第6行维护了事务启动和提交时间戳的映射。这个映射将会在后面用于处理事务状态的查询。

在SI下，一个事务tr读取到的是数据库的一个快照，其包含了所有在tr开始之前提交的事务修改的数据。换句话说，事务tf写入的行，只有在Tc(tf) < Ts(tr)时，才对tr可见。事务tf写入数据库的数据都会以tf的启动时间戳（Ts(tf)）为标签。由于SO上有所有已提交事务的元数据，事务tr可以向SO查询是否Tc(tf) <  Ts (tr)。算法2展示了向SO的查询处理流程。如果事务tf还没有提交，算法返回false；如果tf在tr启动之前已经提交，则返回true，这意味着读取的值对tr可见。

#### Status Oracle in Action
此处我们解释一个基于HBase的SO是如何实现SI的。
图2展示了一次成功提交的流程。因为timestamp oracle集成在SO中，客户端从SO获取启动时间戳。下面是详细的步骤：
单行写：事务tr的写操作直接将新数据写入到HBase中，事务启动时间Ts(tr)作为时间戳
单行读：事务tr的每一个读操作，都需要Ts(tr)之前最后提交的数据。为了这样做，需要从最近的版本开始（假定版本按照升序排序），寻找。为了验证，事务查询SO上的inSnapshot（也就是算法2）。如果对应的事务已经中止，或者当tr开始的时候还未提交，则这个值会被跳过去。根据实现的不同，这个过程可以在客户端或者是服务端运行。
事务提交：在客户端写完数据之后，它尝试向SO发送一个提交请求，里面包含事务ID（这里就是事务的启动时间戳），以及所有的修改列表W。需要注意，如果事务是只读的，W就是空的。

（可选的）单行清理：如果一个事务中止，它之前写的值可以被其他事务忽视，也不需要后续的动作。但是为了有效的利用存储空间，客户端需要通过删除掉中止事务写入的版本来完成清理。如果客户端进行清理时失败，虽然不影响正确性，但是会将中止事务的修改数据遗留在数据服务器上。这些数据将在数据服务器进行垃圾回收时被最终删除。此外，SO也可以主动的使用一个守护进程来周期性的对数据存储进行扫描，并删除中止事务所写入的数据。

#### Serializability
SI防止了写写冲突。为了提供更强的串行化保证，SO也可以修改为支持防止读写冲突。这可能会使得中止率上升，带来了并发性和一致性之间的权衡。

### 3 Omid Design
#### System Model
Omid设计为在一个数据中心内部支持事务。因此我们没有设计支持跨数据中心，如果客户端处于分割的网络中，直到可以访问SO和数据服务器之前，客户端都不能继续工作。事务是由无状态的客户端运行的，这意味着客户端不需要持久化它的状态，客户端的失效也不会影响安全性。我们的设计假定需要一个多版本的数据存储，客户端使用API直接对数据存储进行读写。特别的，Omid的客户端使用下面的API：
	1）put（k，v，t）：使用时间戳t，写主键k的值v
	2）get（t，[t1 .. t2], n）：读取主键k的值的列表，值的版本范围位于t1到t2的区间内，此外n指定了返回值的数量上限
	3）delete（k，v）：删除主键k的版本v的数据。
我们还假定数据存储对数据进行了持久化。

#### Design Overview
在一个大规模分布式数据存储中，事务元数据会超过SO的存储容量。而且为了提升SO处理事务请求的速率，应该避免访问磁盘，而将事务元数据全部放在主存中。有两个限制SO的扩展性的挑战：1）内存容量的限制，2）每个事务必须处理的消息数量。因此我们需要对算法1和算法2进行修改，以支持SO中只有部分元数据保存在内存中。而且为了减轻SO处理查询的压力，事务元数据也可以复制到数据服务器，就像Percolator做的那样。Omid与之不同，将事务元数据复制到了客户端。

#### Memory Limited Capacity
为了检测冲突，Omid检查修改列表W中的每一行r的最后提交时间戳是否早于Ts(ti)。如果结果为真，则ti可以提交，否则ti需要中止。为了进行这一比较，Omid需要数据库内所有行的提交时间戳，这显然不能完全放入内存中。为了解决这一问题，我们将lastCommit列表进行截断，正如图3，Omid只保存可以放入内存中的最后NR个提交的行的状态，但它也维护Tmax（从内存中删除的最大的时间戳）。算法3展示了新的提交处理流程。

如果行r的最后提交时间戳lastCommit(r)已经从内存中删除，SO就无法进行第5行的检查。但是，因为Tmax被定义为比所有从内存中删除的时间戳都要大，包括lastCommit(r)，于是我们有：
	Tmax < Ts(ti)  =>  lastCommit(r) < Ts(ti)
这意味着在冲突事务时间没有时间重叠。否则第2行保守的将事务中止，这意味着有些事务可能被不必要的中止。但如果Pr（Tmax < Ts(ti) ）约等于1，这代表着内存可以在ti的生命周期内保存事务的写列表。下面展示了典型配置下的估算。假定唯一ID需要8个字节，我们可以估计每行的信息所需的空间是32字节，包括行id，开始时间戳，提交时间戳。对于每一GB内存，我们可以在保存32M行的数据。如果每个事务平均修改8行，则最后4M个事务的行可以在内存中，按80K TPS的负载算，那么最后50秒钟的行数据都在内存中，这远远超过了事务的平均提交时间（数十ms）。

算法2需要提交时间戳来判断一个事务写入的版本是否在运行事务的读时间戳中。因此Omid需要修改算法2来处理缺失的提交时间戳。我们认为如果可以区分事务的提交状态（committed、aborted或in progress），就可以不用存储那些老的、无重叠的事务。假定大多数事务都提交了，我们使用committed作为默认的状态：如果一个事务不是中止或者进行中，则它就是已提交了。因此Omid维护abort和uncommitted列表来存储中止的和正在运行的事务。相应的，在事务中止之后，SO将事务id加入abort列表中。当一个事务分配了启动时间戳之后，其id被记录到uncommitted列表中，然后当它提交或中止后，从uncommitted列表中删除。

可以注意到，截断Tc列表的问题，可以通过维护两个新的列表来解决：aborted和uncommitted。但是这两个列表也不能无限的增长。因此我们需要另外的机制来截断这两个列表，同时也不影响安全性。下面我们提供两个技巧来截断这两个列表。

当由于删除数据而增加Tmax中，我们在uncommitted列表中查找启动时间Ts(ti)早于Tmax的事务ti，将其加入到aborted列表中。也就是说，我们将未能及时提交的事务中止掉了。

为了截断aborted列表，当一个已中止事务所写入的版本已经从数据存储中删除后，我们将这个已中止事务从aborted列表中删除。这可以被动触发，在数据存储对老版本进行垃圾回收时。使用上一章提到的可选的清理阶段，客户端也可以主动的删除已中止事务的写入版本，并向SO发送一个clean-up消息。SO而后从aborted列表中删除这个事务id。为了保证对于aborted列表的截断操作不影响其他正在运行的事务，SO将aborted列表的删除推迟到已经启动的事务已经结束后。

算法4描述了验证一个事务tf是否在给定时间戳之前提交的流程。如果Ts(tf)大于Tmax，则事务tf的提交时间不可能被清除出内存，而之前tf也不在committed列表中，这说明了事务tf尚未提交。另外如果tf在a bo rte d列表中，它也同样没有提交。如果这两种情况都没有发生，我们就认为事务tf是一个老的、已提交的事务，可以返回true。
RPC Overhead

SO上的一类负载是接收和发送消息的RPC代价。RPC代价包括处理TCP/IP头，以及应用特定的头，为了减轻RPC的代价，我们尝试减少每个事务收发消息的数量。对于每一个事务，SO都需要收发下面的消息：时间戳请求（TsReq），时间戳响应（TsRes），inSnapshot查询，inSnapshot响应，提交请求，提交响应，以及中止清理。在这中间，inSnapshot是最频繁的消息，至少每个事务的读请求都会有。一种方式是将这个消息的处理卸载到其他节点上去。

按照Percolator的机制，SO上的事务元数据可以被复制到数据服务器上，通过对修改的行写额外的元数据的方式。图4展示了这种架构。这种方式的问题在于需要第二次写提交时间戳，从而给数据服务器带来开销。例如，Percolator在事务处理时会有数秒的开销。与Percolator不同，Omid将SO的状态复制到客户端，图5展示了Omid的架构，由于将SO状态复制到了客户端，每个客户端可以在本地运行算法2来处理inSnapshot查询，而不会导致向SO的RPC。

Omid从下面观察到的现象中得到启发：
	1）处理inSnapshot查询需要SO上至少到事务开始时间的事务元数据。因此在事务开始时间戳之后的提交请求都被安全的忽略了。
	2）由于客户端直接写入HBase，实际的数据不经过SO，它收发的大多是小消息。因此SO是一个CPU密集的服务，SO网卡的带宽利用率很低。
	3）因为TsRes非常短，可以在这个消息中捎带其他的数据。使用适当的压缩算法，上百个事务的元数据可以纳入一个包中。
	4）假定tps是系统整体的吞吐量，N是客户端的数量。如果一个客户端使用平均速率tps/N来运行事务，在来自相同客户端的两个紧连的TsReq之间，SO平均可以处理N个提交请求。在TsRes中捎带新的事务元数据所带来的开销是很小的，如果N不大于1000的话。
因此，Omid可以几乎没有代价的将事务元数据复制到客户端。到客户端Ci的TsRes将会捎带上SO的增量状态数据。理想情况下，这些元数据在压缩后可以被放入TsRes中。对于运行速率较低（小于tps/N）的客户端而言，所需携带的事务元数据将会增大，这将会对SO的扩展到N有负面影响。

更精确的，事务元数据（SO）包含了：1）Tc列表（事务启动和提交时间的映射），2）中止列表，3）Tmax。需要注意lastCommit列表并没有复制到客户端（lastCommit占据了SO上的绝大多数内存），因此客户端所需的内存并不大。
图6展示了Omid中一个成功提交的步骤。如果从数据服务器读取的值尚未提交，客户端需要从数据服务器读取下一个版本。为了避免额外的读取，客户端可以在每个发给数据服务器的读请求中读取最新的nv（nv≥1）个版本。因为数据服务器中的数据也是按版本排序的，读取相邻的版本不会增加多少开销。而且如果选择了一个较小的nv（例如3），所读到的版本可以全部纳入一个包中，而不会增加响应消息的组包开销。

### 4 Implementation
#### SO Data Reliability
在SO失效之后，内存中的数据会丢失。SO的另一个实例需要恢复关键的数据，以继续提供服务。因为lastCommit只用于检测事务之间的冲突，它没有必要在失效后进行恢复，SO可以简单的中止所有在失效前启动但未提交的事务。但是SO必须能够重建Tmax，Tc列表，以及abort列表。uncommitted列表可以从Tc和Aborted来重建：只要没有提交或者没有中止，此事务就在uncommitted列表中。

一个广泛应用的可靠性方案是journaling，其要求将修改持久化到WAL中。WAL也会被复制到多个远程存储设备中，以免在设备失效后出现数据丢失。我们使用了Bookkeeper来实现WAL。因为Omid要求频繁的写WAL，多个写请求可以被批量处理。Bookkeeper的批量处理或者由批量大小触发（累积1KB数据），或者按时间触发（每5ms）

#### Recovery
为了从失效中恢复，SO必须通过从WAL中读数据来重建内存状态。除过最后分配的时间戳之外，还必须恢复的内存状态包括：1）直到Tmax的Tc列表，2）aborted列表。因为分配的时间戳是单调递增的，而且在我们的实现中，获取提交时间戳和写WAL是一个原子操作。因此在WAL中的提交时间戳也是升序的（提交时间戳的获取和WAL是在一起的，和开始时间戳无关）。因此我们将Tc列表的恢复优化为：从WAL的尾部开始读取，直到发现一个小于Tmax的提交时间戳Tc。

aborted列表必须为那些在中止后尚未清理的事务进行恢复，延迟的清理很少见，仅仅是用于客户端失效的情况下。为了减少恢复时间，我们对使用轻量级的快照。SO周期性的对aborted列表进行checkpoint，如果一个中止的项连续出现在两个checkpoint中，那它就包含在checkpoint中。恢复过程将从WAL中读取中止的项，直到读到两个checkpoint。进行checkpoint是在递增Tmax之后，根据Tc列表的大小触发的，其大概对应着需要从WAL中恢复的事务元数据的量。我们的恢复技巧允许快速恢复，而没有传统Checkpoint那样的典型问题：不可忽视的开销和复杂度。总体上，从WAL中需要恢复的量远小于SO的内存量，这是因为lastCommit（没有在WAL中持久化）比Tc列表大一个数量级。为了进一步减少失效恢复时间，一个热备可以持续的从WAL中读取。

#### Client Replica of the SO
为了在本地回答inSnapshot查询，客户端需要维护启动时间戳与提交时间戳的映射，以及aborted列表。客户端使用hashmap来实现事务的启动提交时间映射。hashmap根据每个TsRes捎带的新提交的信息来更新自身。hashmap根据捎带的Tmax来对自己的数据进行垃圾回收。在最近中止的事务之外，捎带的数据还包含了最近清理的abort，用于维护客户端的aborte d列表。

#### Client Startup
因为事务元数据被复制到了客户端，客户端可以在本地处理inSnapshot请求，而不同与SO交互。当一个新的客户端Ci建立与SO的连接时，它接收到一个启动时间戳Tinit，这指示了尚未复制到客户端的元数据的那一部分。如果从数据存储中读到的数据标记为时间戳Ts，而且Tmax < Ts < Tinit，客户端需要查询SO来处理inSnapshot查询。这是唯一需要向SO进行RPC的地方。需要注意，随着系统的运行，Tmax会超过Tinit，这代表着最近的提交信息已经被复制到了客户端。客户端也就不再需要向SO的RPC来处理inSnapshot查询

#### Silent Client
如果一个客户端沉默了太长时间，例如30秒，SO就会向客户端推送事务元数据。这避免了增量的事务数据增长的过大。如果客户端没有对事务元数据做出响应，SO在指定阈值（5秒）后关闭到这一客户端的连接。

#### Computing 增量事务元数据
当计算增量时，是不希望影响到SO的性能的。在事务tw提交后，SO将zip（Ts(tw)，Tc(tw)）追加到commitinfo（一个在所有连接间共享的字节数组），zi p是一个对于开始时间和提交时间的压缩函数。对于每一个打开的连接，SO维护一个指向commitinfo的指针，记录最后一个TsRes消息中发送给客户端的最后一个字节。SO将commitinfo中新增的数据放入下一个TsRes消息中，然后将指针前移到本次的最后一个字节。这一方法的好处在于捎带的数据只在commitinfo中计算一次，发送操作也只涉及SO的内存拷贝。如果需要更多的连接，捎带数据的大小将增加，内存拷贝的代价也会更大。实验显示，Omid可以在可接受的性能下支持到1000个客户端。
#### HBase 垃圾回收
因为HBase为值维护多个版本，它需要一个机制来回收旧的版本。我们对垃圾回收的机制进行了修改，考虑了某些值可能被正在运行的事务读取。SO维护了一个Tmin变量，代表了未提交事务的最小开始时间戳。当垃圾回收时，regionserver向SO获取Tmin变量以及aborted列表。使用这两个信息，垃圾回收可以保证保留没有中止的值，以及开始时间戳早于Tmin的值。
